{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b59215b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from openpyxl import Workbook\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import cross_validate\n",
    "from joblib import dump, load\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "# 停词\n",
    "CATEGORIES_STOPWORDS = ['/.', '/,', '[', ']', '/(', '/)', '\\n', \"/'\", \"/''\", '/``', '/:']\n",
    "CATEGORIES_STOPWORDS_EXTRA = ['/IN', '/DT', '/CC']\n",
    "\n",
    "# 去停词代码\n",
    "STOPLIST_TXT = pd.read_csv('tool word list (stoplist).txt',header=None)[0].values.tolist()\n",
    "PUNCTUATION_STOPWORDS = ['.', ',', '[', ']', '(', ')', '\\n', \"'\", \"''\", '``', ':', ';', '{', '}', '\"']+STOPLIST_TXT\n",
    "\n",
    "# 不去停词\n",
    "# PUNCTUATION_STOPWORDS = ['.', ',', '[', ']', '(', ')', '\\n', \"'\", \"''\", '``', ':', ';', '{', '}', '\"']\n",
    "\n",
    "\n",
    "# Config\n",
    "DATASETS = ['gc', 'nw']\n",
    "MODELS = ['nb', 'dt', 'mlp']\n",
    "# CONFIG = {\n",
    "#     'stopwords': 'all',  # ['all', 'punctuation', 'none']\n",
    "#     'gc': {\n",
    "#         'filename': 'gc_dataset.csv',\n",
    "#         'n_words': 2,\n",
    "#         'vectorizer': 'count',  # ['count', 'tfidf']\n",
    "#         'mlp': {\n",
    "#             'solver': 'sgd',  # ['lbfgs', 'sgd', 'adam']\n",
    "#             'hidden_layer_sizes': (40)\n",
    "#         },\n",
    "#         'dt': {\n",
    "#             'depth': 30\n",
    "#         }\n",
    "#     }\n",
    "# }\n",
    "CONFIG = {\n",
    "    'stopwords': 'all',  # ['all', 'punctuation', 'none']\n",
    "    'gc': {\n",
    "        'filename': 'gc_dataset.csv',\n",
    "        'n_words': 2,\n",
    "        'vectorizer': 'count',  # ['count', 'tfidf']\n",
    "        'mlp': {\n",
    "            'solver': 'adam',  # ['lbfgs', 'sgd', 'adam']\n",
    "            'hidden_layer_sizes': (40,80)\n",
    "            ,'activation':'tanh' #['relu','logistic','tanh']\n",
    "        },\n",
    "        'dt': {\n",
    "            'depth': 30\n",
    "        }},\n",
    "    'nw': {\n",
    "        'filename': 'nw_dataset.csv',\n",
    "        'n_words': 2,\n",
    "        'vectorizer': 'count',  # ['count', 'tfidf']\n",
    "        'mlp': {\n",
    "            'solver': 'adam',  # ['lbfgs', 'sgd', 'adam']\n",
    "            'hidden_layer_sizes': (40,80),\n",
    "            'activation':'tanh' #['relu','logistic','tanh']\n",
    "        },\n",
    "        'dt': {\n",
    "            'depth': 260\n",
    "        }\n",
    "    }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9309a057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grammatical classification extraction\n",
    "def gc_extraction(stopwords=False, extra_stopwords=False, custom_n_words=None):\n",
    "    if custom_n_words is None:\n",
    "        n_words = CONFIG['gc']['n_words']\n",
    "    else:\n",
    "        n_words = custom_n_words\n",
    "    data = []\n",
    "    with open('Corpus for deasmbiguization.txt') as file:\n",
    "        lines = file.readlines()\n",
    "    separator = lines[1]\n",
    "    lines.remove(separator)\n",
    "    for line in lines:\n",
    "        tmp = []\n",
    "        line = line.split(' ')\n",
    "        try:\n",
    "            line.remove('======================================')\n",
    "        except:\n",
    "            pass\n",
    "        if stopwords:\n",
    "            for i in range(len(line)):\n",
    "                stopword_found = False\n",
    "                for stopword in CATEGORIES_STOPWORDS:\n",
    "                    if line[i].find(stopword) != -1:\n",
    "                        stopword_found = True\n",
    "                        break\n",
    "                if not stopword_found and extra_stopwords:\n",
    "                    for stopword in CATEGORIES_STOPWORDS_EXTRA:\n",
    "                        if line[i].find(stopword) != -1:\n",
    "                            stopword_found = True\n",
    "                            break\n",
    "                if not stopword_found:\n",
    "                    for stopword in STOPLIST_TXT:\n",
    "                        if line[i].split(\"/\")[0] == stopword:\n",
    "                            stopword_found = True\n",
    "                            break\n",
    "                if not stopword_found:\n",
    "                    tmp.append(line[i])\n",
    "            line = tmp\n",
    "        nulls = []\n",
    "        for _ in range(n_words):\n",
    "            nulls.append('/VOID')\n",
    "        line = nulls + line + nulls\n",
    "        target_word_found = False\n",
    "        for i in range(len(line)):\n",
    "            if line[i].find('interest_') == 0:\n",
    "                # 取interest的类型数字\n",
    "                category =line[i][9:10]\n",
    "                line = line[i - n_words:i + n_words + 1]\n",
    "                target_word_found = True\n",
    "                break\n",
    "            elif line[i].find('interests_') == 0:\n",
    "                category = line[i][10:11]\n",
    "                line = line[i - n_words:i + n_words + 1]\n",
    "                line.pop(n_words)\n",
    "                target_word_found = True\n",
    "                break\n",
    "        if target_word_found:\n",
    "            line.pop(n_words)\n",
    "            for i in range(len(line)):\n",
    "                try:\n",
    "                    line[i] = line[i].split('/')[1]\n",
    "                except:\n",
    "                    line[i] = 'VOID'\n",
    "            line = ' '.join(line)\n",
    "            line = line.replace('VOID', '')\n",
    "            line = [category, line]\n",
    "            data.append(line)\n",
    "    header = ['label', 'features']\n",
    "    with open(CONFIG['gc']['filename'], 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(header)\n",
    "        writer.writerows(data)\n",
    "    print(CONFIG['gc']['filename'] + ' generated')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f54044d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc_extraction()\n",
    "# whole_sentence_extraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e81fe6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datasets = ['gc']\n",
    "# datasets = ['gc','nw']\n",
    "models =  ['naive_bayes', 'decision_tree','random_forest', 'svm','mlp']\n",
    "# models =  ['mlp']\n",
    "for current_dataset in datasets:\n",
    "    print(CONFIG[current_dataset])\n",
    "    data = pd.read_csv(CONFIG[current_dataset]['filename'], header=None)\n",
    "    print('Dataset: ' + current_dataset)\n",
    "    y = data[0].values\n",
    "    X = data[1]\n",
    "    if CONFIG[current_dataset]['vectorizer'] == 'count':\n",
    "        vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "    else:\n",
    "        vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "    X = vectorizer.fit_transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "    for current_model in models:\n",
    "        if current_model == 'naive_bayes':\n",
    "            nb = MultinomialNB()\n",
    "            nb.fit(X_train, y_train)\n",
    "            y_pred = nb.predict(X_test)\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred, average='macro')\n",
    "            print('naive_bayes Accuracy: ' + str('{:.4f}'.format(acc)))\n",
    "        elif current_model == 'decision_tree':\n",
    "            dt = DecisionTreeClassifier(max_depth=CONFIG[current_dataset]['dt']['depth'])\n",
    "            dt.fit(X_train, y_train)\n",
    "            y_pred = dt.predict(X_test)\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred, average='macro')\n",
    "            print('decision_tree Accuracy: ' + str('{:.4f}'.format(acc)))\n",
    "        elif current_model == 'random_forest':\n",
    "            clf = RandomForestClassifier(max_depth=12, random_state=0)\n",
    "            clf = clf.fit(X_train, y_train)\n",
    "            y_test = clf.predict(X_test)  \n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred, average='macro')\n",
    "            print('random_forest Accuracy: ' + str('{:.4f}'.format(acc)))\n",
    "\n",
    "        elif current_model == 'svm':\n",
    "            kernels=['sigmoid','rbf']\n",
    "            clf=svm.SVC(kernel=kernels[1]).fit(X_train, y_train)\n",
    "            y_test = clf.predict(X_test)\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred, average='macro')\n",
    "            print('svm Accuracy: ' + str('{:.4f}'.format(acc)))\n",
    "\n",
    "        elif current_model == 'mlp':\n",
    "            mlp = MLPClassifier(hidden_layer_sizes=CONFIG[current_dataset]['mlp']['hidden_layer_sizes']\n",
    "                                , max_iter=3000\n",
    "                                , solver=CONFIG[current_dataset]['mlp']['solver']\n",
    "            ,activation=CONFIG[current_dataset]['mlp']['activation'])\n",
    "            mlp.fit(X_train, y_train)\n",
    "            y_pred = mlp.predict(X_test)\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            f1 = f1_score(y_test, y_pred, average='macro')\n",
    "            print('mlp Accuracy: ' + str('{:.4f}'.format(acc)))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
